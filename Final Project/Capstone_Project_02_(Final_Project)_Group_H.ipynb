{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabdulghofur142/Data-Mining/blob/main/Final%20Project/Capstone_Project_02_(Final_Project)_Group_H.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Capstone Project 02 (Final Project) - Group H***\n",
        "\n",
        "| NIM | Nama | Deskripsi |\n",
        "| -------- | -------- | -------- |\n",
        "|11210940000073   | Tonny Pramudya Bagus Santoso | Mencari StopWords, Presentasi|\n",
        "|11220940000020  | Muhammad Abdul Ghofur |Scrapping, Mencari StopWords, Normalisasi Data Teks, Labelling, Visualisasi, Interpretassi|\n",
        "|11220940000047   | Rajwaa Warda Yunenda Putri      |Mencari StopWords, Visualisasi, PPT, dan Interpretasi|\n",
        "|11220940000052   | Tsabita Salma    |Labelling Sentimen, PPT, dan Interpretasi|\n",
        "|11220940000075   | Raden Arinal Haque |Labelling Sentimen, Presentasi|\n"
      ],
      "metadata": {
        "id": "5A8EBk-KCrdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latar Belakang"
      ],
      "metadata": {
        "id": "jtIjkGKERMPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada bulan September, saat Timnas Indonesia mulai berlaga di babak ketiga kualifikasi Piala dunia 2026, isu naturalisasi pemain menjadi perbincangan hangat. Kebijakan ini bertujuan meningkatkan performa tim keturunan sebagai Warga Negara Indonesia (WNI).\n",
        "\n",
        "\n",
        "Meskipun dianggap strategis untuk meningkatkan kualitas permainan, naturalisasi memicu pro dan kontra. Sebagian mendukungnya sebagai upaya mendongkrak prestasi, sementara yang lain mengkritik karena dianggap mengabaikan pembinaan pemain lokal.\n",
        "\n",
        "\n",
        "Diskusi ini ramai di Twitter (X), mencerminkan opini publik yang beragam. Analisis sentimen terhadap percakapan ini dapat memberikan wawasan penting bagi pemangku kebijakan untuk menyusun strategi sepak bola nasional yang lebih baik."
      ],
      "metadata": {
        "id": "qhViEg9ZRVJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rumusan Masalah"
      ],
      "metadata": {
        "id": "G5Zz8rVBRdDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Permasalahan**\n",
        "1. Apa isu-isu utama yang terkait dengan Naturalisasi Pemain Timnas Indonesia?\n",
        "2. Bagaimana sentimen publik terhadap Naturalisasi Pemain Timnas Indonesia?\n",
        "3. Bagaimana  pola percakapan tentang hal ini di media sosial?\n",
        "4. Bagaimana respons netizen terhadap Naturalisasi Pemain Timnas Indonesia?"
      ],
      "metadata": {
        "id": "0F9CeONBRfwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Periode Pengumpulan Data**\n",
        "\n",
        "1-30 September 2024"
      ],
      "metadata": {
        "id": "5OZELrhPRw_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sumber Data**\n",
        "\n",
        "Twitter (X)"
      ],
      "metadata": {
        "id": "8Z8hybNDR86P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kata Kunci**\n",
        "* naturalisasi bola indonesia\n",
        "* naturalisasi timnas indonesia\n",
        "* pemain keturunan indonesia"
      ],
      "metadata": {
        "id": "8NywKV0pSD5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "5SWDY4JCCj5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install unidecode\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install wordcloud matplotlib\n",
        "!pip install squarify"
      ],
      "metadata": {
        "id": "dRG-XkHD0KBO",
        "outputId": "ec2273ec-b115-4cd3-e2a5-746a3cc16694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from html import unescape\n",
        "import pandas as pd\n",
        "from unidecode import unidecode # Import the unidecode function\n",
        "from textblob import TextBlob # Import the TextBlob class\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import requests\n",
        "import json\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.lines import Line2D\n",
        "from wordcloud import WordCloud\n",
        "import re, operator, numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import squarify\n",
        "\n"
      ],
      "metadata": {
        "id": "2Je_D_-0i6a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mempersiapkan Data"
      ],
      "metadata": {
        "id": "v-h0yuO7seKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrapping Data Twitter (X)"
      ],
      "metadata": {
        "id": "GHspZAng3RQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada Proses Scrapping data dari twitter kami menggunakan library tweet-harvest dari Helmi Satria pada sumber link berikut:\n",
        "- https://helmisatria.com/blog/crawl-data-twitter-menggunakan-tweet-harvest/\n",
        "- https://colab.research.google.com/drive/1jQhAGKanGZ290rIaf06705xmhAB3vpD9\n",
        "- https://www.youtube.com/watch?v=OK6b5vvjRzY"
      ],
      "metadata": {
        "id": "FcKywQl87GJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# auth token yang dimiliki tiap pengguna twitter untuk akses\n",
        "twitter_auth_token = \"....\" # masukkan auth token twitter di sini\n",
        "\n",
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v\n",
        "\n",
        "\n",
        "## Limit 1000 tweet\n",
        "limit = 1000\n",
        "## berikut list keyword kami\n",
        "listkey = ['naturalisasi bola indonesia',\n",
        "            'naturalisasi timnas indonesia',\n",
        "            'pemain keturunan indonesia']\n",
        "```\n"
      ],
      "metadata": {
        "id": "IfQ0fVHZ7hmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kami menggunakan 3 keyword dan melakukan beberapa kali scrapping pada tiap keyword, yakni di setiap 10 hari pada bulan september untuk memastikan semua tweet dalam bulan tersebut dapat diambil. Dengan demikian terdapat setidaknya terdapat 9 pengambilan\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Contoh pengambilan yang kami lakukan\n",
        "## Pengambilan Keyword Pertama di 10 hari pertama bulan September\n",
        "filename1 = 'naturalisasi_bola_indonesia_1.csv'\n",
        "search_keyword1 = 'naturalisasi bola indonesia until:2024-09-11 since:2024-09-01'\n",
        "\n",
        "!npx --yes tweet-harvest@2.6.1 -o \"{filename1}\" -s \"{search_keyword1}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEJh2lr-ALp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kami lakukan proses penggabungan dataframe dan Ekspor ke Github\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Mencari semua file CSV di dalam folder 'tweets-data'\n",
        "csv_files = glob.glob('tweets-data/*.csv') # list nama file yang disimpan\n",
        "csv_files.sort()  # Use the sort() method of the list object\n",
        "\n",
        "n = 0\n",
        "for filename in csv_files: ## Menggabungkan file csv dalam bentuk dataframe\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  if n == 0:\n",
        "    df = pd.read_csv(filename, delimiter=\",\")\n",
        "    n = 1\n",
        "  else:\n",
        "    df2 = pd.read_csv(filename, delimiter=\",\")\n",
        "    df = pd.concat([df, df2])\n",
        "    n+=1\n",
        "\n",
        "# Membuang duplikat berdasarkan id_str\n",
        "df.drop_duplicates(subset=['id_str'], keep='first', inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Ekspor Data Hasil Scrapping\n",
        "from google.colab import files\n",
        "namafile = \"DATAX_Naturalisasi_fix.csv\"\n",
        "df.to_csv(namafile, index=False)\n",
        "files.download(namafile)\n",
        "\"Exported\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QmEfBtjv_E5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data Hasil Scrapping"
      ],
      "metadata": {
        "id": "mROpqQFhCpPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelumnya pada link berikut\n",
        "- https://docs.google.com/spreadsheets/d/1W2n8ny7i5A74D0c-iB-gQUG_eeaZmHEAQgixtVVokmg/edit?usp=sharing\n",
        "\n",
        "Kami telah melakukan proses pelabelan sentimen (3:Positif, 1:Negatif, dan 2:Netral) pada setiap tweet sebelum data ini diunggah ke GitHub dan diimpor ke sini. Langkah ini dilakukan untuk mendukung analisis sentimen publik terkait topik naturalisasi Timnas Indonesia."
      ],
      "metadata": {
        "id": "wcxlYqpNBZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data hasil scrapping twitter (X)\n",
        "import pandas as pd\n",
        "\n",
        "file_ = 'data/Data_X_Naturalisasi_Timnas_Indonesia_Sep.csv'\n",
        "try: #Loading Locally\n",
        "    df = pd.read_csv(file_)\n",
        "except Exception as err_:\n",
        "    print(err_,\" Trying to load data from GitHub.\")\n",
        "    !mkdir data\n",
        "    !wget -P data/ https://raw.githubusercontent.com/mabdulghofur142/Data-Mining/refs/heads/main/Final%20Project/Data_X_Naturalisasi_Timnas_Indonesia_Sep.csv\n",
        "    df = pd.read_csv(file_)\n",
        "\n",
        "print(df.shape)\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "zCtkwbXcOi4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jumlah Tweet\n",
        "print(\"Jumlah Tweet yang kami peroleh:\", df.shape[0],\" Tweet\")"
      ],
      "metadata": {
        "id": "LdUEDbebCPnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Zu7w9jEMjAYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalisasi data teks"
      ],
      "metadata": {
        "id": "MZSL6UxACulY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus kolom yang tidak digunakan\n",
        "df.drop(columns=['id_str','image_url','in_reply_to_screen_name','lang','conversation_id_str'],inplace=True)"
      ],
      "metadata": {
        "id": "n3rljzLtjGxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menghilangkan hal yang tidak diperlukan di dalam teks"
      ],
      "metadata": {
        "id": "rjpjP3dl41wn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QozK0Xk7ObrQ"
      },
      "outputs": [],
      "source": [
        "# Fungsi yang digunakan\n",
        "import re\n",
        "def get_hashtag(df,teks): # untuk mendapatkan hashtag\n",
        "    hashtag = [username[1:] for username in re.findall(r'#\\w+',teks)]\n",
        "    return hashtag\n",
        "def get_username(df,teks): # untuk mendapatkan list user yang mention atau reply\n",
        "    usernames = [username[1:] for username in re.findall(r'@\\w+',teks)]\n",
        "    return usernames\n",
        "\n",
        "def get_teks(df,teks): # bersihkan teks dari mention dan hashtag\n",
        "    teks = re.sub(r'@\\w+','',teks).strip()\n",
        "    teks = re.sub(r'#\\w+','',teks).strip()\n",
        "    return teks\n",
        "\n",
        "def remove_links_email(teks): # besihkan teks dari link dan email\n",
        "    docx = teks.strip()\n",
        "    urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
        "    docx = re.sub(urlPattern,' ', docx) # Remove links\n",
        "    docx = re.sub(emailPattern,' ', docx) # Remove email\n",
        "    return docx\n",
        "\n",
        "def unidecode_text(text): # fungsi untuk encode format ASCII dan membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
        "    return unescape(unidecode(text))\n",
        "\n",
        "\n",
        "df['username_in_teks'] = df['full_text'].apply(lambda x: get_username(df,x))\n",
        "df['hastag_in_teks'] = df['full_text'].apply(lambda x: get_hashtag(df,x))\n",
        "df['teks_new'] = df['full_text'].apply(lambda x: get_teks(df,x))\n",
        "df['teks_new'] = df['teks_new'].apply(lambda x: remove_links_email(x))\n",
        "df['teks_new'] = df['teks_new'].apply(lambda x: unidecode_text(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penanganan Slang/Singkatan\n"
      ],
      "metadata": {
        "id": "jyJ2xAg14JBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pada Link berikut:\n",
        "- https://colab.research.google.com/drive/1hCJ8mrCtubMbimKWU1LLRPxoPdXtV9Gh?usp=sharing\n",
        "\n",
        "Kami gabungkan slang singkatan yang dikumpulkan oleh kami dengan yang ada pada link github berikut:\n",
        "- https://raw.githubusercontent.com/taudataanalytics/eLearning/refs/heads/master/data/slang.txt\n",
        "\n",
        "lalu di upload kembali ke dalam Github untuk kami gunakan"
      ],
      "metadata": {
        "id": "Jb8kzjQV5K52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mengimport kumpulan slang/singkatan dari internet dan yang sudah kami buat\n",
        "# URL of the raw JSON file from GitHub\n",
        "url = 'https://raw.githubusercontent.com/mabdulghofur142/Data-Mining/main/Final%20Project/slangS_Final_Project_Group_H.json'\n",
        "\n",
        "# Fetch the JSON data from GitHub\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Load the content into a Python dictionary\n",
        "    slangS = response.json()\n",
        "    print(\"Dictionary loaded successfully:\")\n",
        "    # print(slangS)\n",
        "    print(len(slangS))\n",
        "else:\n",
        "    print(\"Failed to fetch the file, status code:\", response.status_code)\n",
        "\n",
        "slangS.pop('et')\n",
        "slangS.pop('is')\n",
        "slangS.pop('rg')\n",
        "slangS.pop('sty')\n",
        "slangS.pop('as')\n",
        "slangS.pop('euforia')\n",
        "slangS.pop('Euforia')\n",
        "\n",
        "slang_nama = {'STY':'shin tae-yong',\n",
        "              'PG':'peter gontha',\n",
        "              'RG':'rocky gerung',\n",
        "              'RK':'Ridwan Kamil',\n",
        "              'IS':'indra sjafri',\n",
        "              'ET':'erick thohir',\n",
        "              'AS':'Amerika Serikat',\n",
        "              }\n"
      ],
      "metadata": {
        "id": "22glbAqlyiyP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan penanganan slang/singkatan\n",
        "def replace_slang(text,dict_slang):\n",
        "    # Mengganti frasa menggunakan regex\n",
        "    for slang, pengganti in dict_slang.items():\n",
        "        text = re.sub(r'\\b' + re.escape(slang) + r'\\b', pengganti, text) # versi tanpa ignorecase, sensitif dengan huruf kapital atau kecil\n",
        "    return text\n",
        "\n",
        "# dikarenakan ada beberapa perbedaan format penulisan dalam dictionary slang kami maka dilakukan beberapa proses,\n",
        "df['teks_slang1'] = df['teks_new'].apply(lambda x: replace_slang(x, slang_nama)) # replace singkatan nama yang sensitive case\n",
        "df['teks_slang2'] = df['teks_slang1'].apply(lambda x: replace_slang(x, slangS)) # replace singkatan yang sensitive case\n",
        "df['teks_slang3'] = df['teks_slang2'].apply(lambda x: replace_slang(x.lower(), slangS)) # replace singkatan dengan lower"
      ],
      "metadata": {
        "id": "m11jCmg4GXgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghapus angka di akhir kata\n",
        "def remove_trailing_numbers(text):\n",
        "    return re.sub(r'\\d+\\b', '', text)\n",
        "\n",
        "df['teks_slang3'] = df['teks_slang3'].apply(lambda x: remove_trailing_numbers(x))"
      ],
      "metadata": {
        "id": "wHblcRQLPlZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jumlah singkatan STY sebelum penanganan slang/singkatan\n",
        "print('Singkatan STY sebelum: ',df['teks_new'].str.contains('STY').sum())\n",
        "print('Singkatan lvl sebelum: ',df['teks_new'].str.contains('lvl').sum())"
      ],
      "metadata": {
        "id": "Les9fHzFKMJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jumlah singkatan STY dan lvl setelah penanganan slang/singkatan\n",
        "print('Singkatan STY setelah: ',df['teks_slang3'].str.contains('STY').sum())\n",
        "print('Singkatan lvl setelah: ',df['teks_slang3'].str.contains('lvl').sum())"
      ],
      "metadata": {
        "id": "fPQvbU_VAfAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi bersihkan simbol tak perlu\n",
        "def remove_symbol(teks):\n",
        "    docx = teks.strip()\n",
        "    docx = re.sub(r'[^\\w]', ' ', docx) # Remove Symbol, keep \"_\" ... recommended\n",
        "    return docx\n",
        "\n",
        "df['teks_rapih'] = df['teks_slang3'].apply(lambda x: remove_symbol(x))"
      ],
      "metadata": {
        "id": "o3y4iKjp5Rac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penanganan Stopword"
      ],
      "metadata": {
        "id": "j6y5YaF65Rij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopword sastrawi\n",
        "factory = StopWordRemoverFactory() # Create an instance of the StopWordRemoverFactory\n",
        "Sastrawi_StopWords_id = factory.get_stop_words()\n",
        "\n",
        "# print(Sastrawi_StopWords_id)\n",
        "print(len(Sastrawi_StopWords_id))\n",
        "\n",
        "Sastrawi_StopWords_id = set(Sastrawi_StopWords_id)"
      ],
      "metadata": {
        "id": "_V0n4otl-E1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopword dari nltk\n",
        "# Mengambil stopwords dalam bahasa Inggris\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Mengambil stopwords dalam bahasa Indonesia\n",
        "stop_words_id = set(stopwords.words('indonesian'))\n",
        "\n",
        "\n",
        "# print(\"Stopwords Bahasa Inggris:\", stop_words)\n",
        "# print(\"Stopwords Bahasa Indonesia:\", stop_words_id)\n",
        "print(len(stop_words))\n",
        "print(len(stop_words_id))"
      ],
      "metadata": {
        "id": "ZZ2XnCt0_LgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = Sastrawi_StopWords_id | stop_words | stop_words_id # gabung semua stopwords\n",
        "len(stopwords)"
      ],
      "metadata": {
        "id": "L7NpzDsm_XVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi bersihkan stopwords\n",
        "def clean_text(text, stopwords):\n",
        "    # Mengubah teks menjadi huruf kecil\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenisasi menggunakan TextBlob\n",
        "    tokens = TextBlob(text).words\n",
        "\n",
        "    # Menghapus stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "    # Menggabungkan kembali token menjadi teks\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "IRmF6nF_AN8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_teks'] = df['teks_rapih'].apply(lambda x: clean_text(x, stopwords))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Ru-n__TFA9Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematisasi"
      ],
      "metadata": {
        "id": "G5eRUmHt5gC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat stemmer Sastrawi untuk proses Lemmatisasi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# fungsi untuk Lemmatisasi\n",
        "def stem_text(text):\n",
        "    # Melakukan Lemmatisasi  pada teks\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "df['stem_teks'] = df['clean_teks'].apply(lambda x: stem_text(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SuZrwdcNLDuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekspor Data Clean"
      ],
      "metadata": {
        "id": "5gBQoObE2hel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor hasil data bersih\n",
        "df.to_csv('data/data_final_all_fix.csv',index=False)"
      ],
      "metadata": {
        "id": "QLQyv7lZEee3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data diekspor dan akan diunggah ke GitHub untuk memudahkan proses visualisasi, sehingga tidak perlu menjalankan ulang seluruh proses dari data mentah."
      ],
      "metadata": {
        "id": "oY9zE6Qz2tpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisasi dan Interpretasi\n"
      ],
      "metadata": {
        "id": "eAJN9FLM_YwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data Clean"
      ],
      "metadata": {
        "id": "Lo4GMVx6_lF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAaWyAOPKrhn"
      },
      "outputs": [],
      "source": [
        "# Import data yang telah di ekspor\n",
        "import pandas as pd\n",
        "\n",
        "file_ = 'data/data_final_all_fix.csv'\n",
        "try: #Loading Locally\n",
        "    df_final = pd.read_csv(file_)\n",
        "except Exception as err_:\n",
        "    print(err_,\" Trying to load data from GitHub.\")\n",
        "    !mkdir data\n",
        "    !wget -P data/ https://raw.githubusercontent.com/mabdulghofur142/Data-Mining/refs/heads/main/Final%20Project/data_final_all_fix.csv\n",
        "    df_final = pd.read_csv(file_)\n",
        "\n",
        "print(df_final.shape)\n",
        "df_final.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jumlah Tweet\n",
        "print(\"Jumlah Tweet yang kami peroleh:\", df_final.shape[0],\" Tweet\")"
      ],
      "metadata": {
        "id": "5spxmFtF9cgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.info()"
      ],
      "metadata": {
        "id": "mud-Gk0h_7p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus kolom yang tidak digunakan\n",
        "df_final = df_final.drop(columns=['teks_slang1', 'teks_slang2', 'teks_slang3', 'user_id_str', 'username_in_teks'])\n",
        "\n",
        "# Mapping angka ke label sentimen\n",
        "sentiment_mapping = {1: 'Negatif', 2: 'Netral', 3: 'Positif'}\n",
        "\n",
        "# Menambahkan kolom label sentimen berdasarkan mapping\n",
        "df_final['sentiment_label'] = df_final['sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Mengonversi kolom 'created_at' ke format datetime\n",
        "df_final['created_at'] = pd.to_datetime(df_final['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
        "\n",
        "# Mengonversi string menjadi list\n",
        "df_final['hastag_in_teks'] = df_final['hastag_in_teks'].str.lower().apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "q-kBqw4F_XZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.info()"
      ],
      "metadata": {
        "id": "mE3vm-h2Bg9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengurutkan data berdasarkan Waktu\n",
        "df_final = df_final.sort_values(by='created_at')\n",
        "df_final.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "7jyoJwaGTaYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung total engagement per tweet\n",
        "df_final['engagement'] = df_final['quote_count'] + df_final['reply_count'] + df_final['retweet_count'] + df_final['favorite_count']\n"
      ],
      "metadata": {
        "id": "F_i8IG6leXT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisasi (Analisis Tweet)"
      ],
      "metadata": {
        "id": "ud8OTPY3J8JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jumlah Tweet Harian"
      ],
      "metadata": {
        "id": "haAgUI9CTHp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyiapkan kolom untuk format tanggal harian\n",
        "df_final['tanggal'] = df_final['created_at'].dt.strftime('%d %b')\n",
        "# Menghitung banyak tweet perhari\n",
        "tweet_per_hari = df_final.groupby('tanggal').size()\n",
        "# Convert the result into a DataFrame and rename the column\n",
        "tweet_per_hari = tweet_per_hari.reset_index(name='size')"
      ],
      "metadata": {
        "id": "7XLQIl5ETGic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with one axis\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Barplot\n",
        "sns.barplot(x='tanggal', y='size', data=tweet_per_hari, ax=ax, color='salmon')\n",
        "\n",
        "# Lineplot on top of the barplot\n",
        "sns.lineplot(x='tanggal', y='size', data=tweet_per_hari, ax=ax, color='red', marker='o')\n",
        "\n",
        "# Annotate the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',  # Menghapus desimal dan mengubah ke integer\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom',  # Posisikan tepat di atas bar\n",
        "                fontsize=8, color='black',  # Ukuran font lebih kecil\n",
        "                xytext=(0, 5), textcoords='offset points')  # Jarak sedikit di atas bar\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Tanggal')\n",
        "ax.set_ylabel('Jumlah Tweet')\n",
        "ax.set_title('Barplot dan Lineplot Jumlah Tweet per Hari')\n",
        "\n",
        "# Display the plot\n",
        "plt.xticks(rotation=45)  # Rotate the x-axis labels for better visibility\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oU6aTuUQTqOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "- Isu naturalisasi pemain timnas mulai ramai dibahas khususnya di media sosial Twitter (X) pada bulan September, yang dipicu oleh laga kualifikasi Piala Dunia (5 dan 10 September).\n",
        "\n",
        "- Lonjakan besar terjadi pada 12 September (162 tweet) dan 13 September (282 tweet) karena statement yang dikeluarkan oleh peter gontha menunjukkan ketidaksetujuan terhadap kebijakan PSSI terkait naturalisasi Timnas."
      ],
      "metadata": {
        "id": "CQyk4CSEUF0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tagar Populer"
      ],
      "metadata": {
        "id": "60_rb6Ixo7de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan semua hashtag ke dalam satu list besar\n",
        "all_hashtags = [hashtag for sublist in df_final['hastag_in_teks'] for hashtag in sublist]\n",
        "\n",
        "# Menghitung frekuensi hashtag\n",
        "hashtag_counts = Counter(all_hashtags)\n",
        "\n",
        "# Mengonversi ke DataFrame untuk visualisasi\n",
        "hashtag_df = pd.DataFrame(hashtag_counts.items(), columns=['Hashtag', 'Count'])\n",
        "\n",
        "# Mengambil 30 hashtag terbanyak\n",
        "hashtag_df = hashtag_df.nlargest(30, 'Count')\n",
        "\n",
        "# Membuat treemap\n",
        "plt.figure(figsize=(15, 6))\n",
        "squarify.plot(sizes=hashtag_df['Count'], label=hashtag_df['Hashtag'], alpha=0.7, color=plt.cm.viridis(hashtag_df['Count'] / max(hashtag_df['Count'])))\n",
        "\n",
        "# Menambahkan judul dan label\n",
        "plt.title('Top 30 Hashtags Treemap')\n",
        "plt.axis('off')  # Menonaktifkan axis\n",
        "\n",
        "# Tampilkan plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDZx21niCEB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "Tagar populer #timnasday, #naturalisasi, dan #timnasindonesia mencerminkan pembahasan isu naturalisasi pemain dalam konteks antusiasme dan dukungan terhadap timnas Indonesia selama pertandingan.\n"
      ],
      "metadata": {
        "id": "B5od7uSXUNLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisasi Jaringan Sosial Netizen X"
      ],
      "metadata": {
        "id": "owYLLxDDpCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "# Draw the Tweet Graph\n",
        "G=nx.Graph()\n",
        "for i, tweet in tqdm(df_final.iterrows()):\n",
        "    if tweet.username not in G.nodes():\n",
        "        G.add_node(tweet.username)\n",
        "    mentionS =  re.findall(\"@([a-zA-Z0-9]{1,15})\", tweet['full_text'])\n",
        "    for mention in mentionS:\n",
        "        if \".\" not in mention: #skipping emails\n",
        "            usr = mention.replace(\"@\",'').strip()\n",
        "            if usr not in G.nodes():\n",
        "                G.add_node(usr)\n",
        "            G.add_edge(tweet.username, usr)\n",
        "Nn=G.number_of_nodes();Ne=G.number_of_edges()\n",
        "print('Finished. There are %d nodes and %d edges in the Graph.' %(Nn,Ne))"
      ],
      "metadata": {
        "id": "0xJSv2lxM-5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Ekspor graf ke file GraphML untuk nantinya diproses dalam software Gephi\n",
        "nx.write_graphml(G, \"twitter_graph_timnas.graphml\")\n"
      ],
      "metadata": {
        "id": "FuZw2lPbNOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hasil output dari Gephi di upload ke dalam link drive pada url berikut\n",
        "# https://drive.google.com/file/d/1MjrTWYewA_JNedDIQK4WYpCO5H6d-aRk/view?usp=drive_link\n",
        "\n",
        "import requests\n",
        "# Tautan unduhan langsung\n",
        "id = \"1MjrTWYewA_JNedDIQK4WYpCO5H6d-aRk\"\n",
        "url = f\"https://drive.google.com/uc?id={id}\"\n",
        "output_path = \"/content/graph.svg\"\n",
        "\n",
        "# Unduh file\n",
        "response = requests.get(url)\n",
        "with open(output_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(f\"File SVG berhasil diunduh: {output_path}\")"
      ],
      "metadata": {
        "id": "D9rjl52lp_O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan Hasil Output\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Path ke file SVG\n",
        "svg_path = \"/content/graph.svg\"\n",
        "\n",
        "# Baca isi file SVG\n",
        "with open(svg_path, \"r\") as file:\n",
        "    svg_content = file.read()\n",
        "\n",
        "# HTML dengan slider dan SVG otomatis berada di tengah\n",
        "html_content_with_centering = f\"\"\"<div style=\"border: 1px solid black; width: 200vh; height: 175vh; display: flex; flex-direction: column; align-items: center; justify-content: center; overflow: hidden;\">\n",
        "    <div id=\"svgContainer\" style=\"display: flex; justify-content: center; align-items: center; transform: scale(0.45); transform-origin: center;\">\n",
        "        {svg_content}\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Tampilkan dengan centering dan margin top\n",
        "display(HTML(html_content_with_centering))"
      ],
      "metadata": {
        "id": "73wDcsGh0gUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "Berdasarkan analisis jaringan, beberapa akun Twitter (X) dengan degree tertinggi sering disebut atau di-reply dalam diskusi terkait topik naturalisasi pemain sepak bola. Akun-akun tersebut dapat dikelompokkan berdasarkan peran mereka:\n",
        "\n",
        "- Akun media: Akun seperti idextratime, indostransfer, gilabola, panditfootball, sukanyepak, dan kegblgunfaedh berperan sebagai pusat penyebaran informasi. Mereka menjadi referensi utama bagi Netizen.\n",
        "\n",
        "- Tokoh sentral: ErickThohir dan PeterGontha sering menjadi subjek pembicaraan.\n",
        "\n",
        "- Akun pribadi aktif: Akun seperti pangeransiahaan, pray_thekid, dan FahmiAgustian  memberikan pandangan atau tanggapan yang menarik perhatian diskusi.\n",
        "\n",
        "Narasi yang muncul dalam pembicaraan ini mencakup beberapa tema utama, yaitu:\n",
        "\n",
        "1. Rasa bangga masyarakat terhadap performa Timnas Indonesia, terutama setelah kehadiran pemain naturalisasi.\n",
        "2. Kritik terhadap kebijakan naturalisasi, termasuk dari pengamat sepak bola dan Peter Gontha, yang mempertanyakan langkah PSSI di bawah kepemimpinan Erick Thohir.\n",
        "3. Informasi tentang calon pemain baru, yang menjadi daya tarik utama bagi diskusi di antara pengguna Twitter.\n",
        "4. Perbandingan internasional, di mana muncul opini bahwa Timnas Indonesia memiliki keunggulan dibandingkan negara lain.\n"
      ],
      "metadata": {
        "id": "RTK8w_erUenm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diperoleh beberapa akun dengan mention terbanyak (dilihat dari degree yang besar)\n",
        "list_akun = ['kegblgnunfaedh','erickthohir','Indostransfer','gilabola', 'pangeransiahaan','panditfootball','idextratime','sukanyepak','PeterGontha','FahmiAgustian','pray'] # akun yang ingin diperiksa\n",
        "\n",
        "# Mencari tweet yang melakukan mention yang kemudian akan diperoleh tweet apa yang di-reply oleh Netizen X\n",
        "for i in range(len(list_akun)):\n",
        "    print(f\"[{list_akun[i]}]\")\n",
        "    nama_akun = list_akun[i]\n",
        "    isi_tweet = df_final[df_final['full_text'].str.contains('@'+nama_akun, case=False, na=False)].head(5) # head untuk kustomisasi jumlah tweet yang ditarik\n",
        "    listtweet = list(isi_tweet['full_text'])\n",
        "    listurltweet = list(isi_tweet['tweet_url'])\n",
        "    for j in range(len(listtweet)):\n",
        "       print(f\"{j+1}. Link [{listurltweet[j]}] : {listtweet[j]}\")"
      ],
      "metadata": {
        "id": "lVaNCweZbYA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisasi (Analisis Sentimen)"
      ],
      "metadata": {
        "id": "E6kC3sMBHQkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dataframe baru tanpa duplikasi berdasarkan teks untuk analisis sentimen\n",
        "# Agar tidak dipengaruhi oleh adanya spam ataupun buzzer\n",
        "df2 = df_final.drop_duplicates(subset=['teks_rapih'])\n",
        "df2.shape"
      ],
      "metadata": {
        "id": "Zm49Ny2gVxj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pie chart sentimen"
      ],
      "metadata": {
        "id": "bWv4iNxAZZiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung distribusi kategori\n",
        "category_counts = df2['sentiment_label'].value_counts()\n",
        "\n",
        "# Membuat pie chart\n",
        "category_counts.plot.pie(autopct='%1.1f%%', colors=['lightgreen', 'skyblue', 'lightcoral'])\n",
        "\n",
        "# Menambahkan judul\n",
        "plt.title('Distribusi Kategori')\n",
        "\n",
        "# Menampilkan chart\n",
        "plt.ylabel('')  # Menghilangkan label y agar lebih rapi\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AvdJ7CxUZFt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "- Positif (34.3%)\n",
        "1. Keturunan Indonesia memiliki hak untuk membela Timnas\n",
        "2. Proses naturalisasi diakui secara aturan\n",
        "3. Naturalisasi menambah kualitas Timnas\n",
        "\n",
        "- Negatif (14.4%)\n",
        "1. Kegagalan dalam pembinaan pemain lokal\n",
        "2. Ketergantungan pada pemain asing\n",
        "3. Ketidakseimbangan antara pemain lokal dan pemain naturalisasi"
      ],
      "metadata": {
        "id": "8c7KkF1PU4rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Cloud"
      ],
      "metadata": {
        "id": "PL_bkL-LVMQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengolah kembali stopword, karena terdapat kata-kata baru yang mengganggu Word Cloud\n",
        "# Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan sentimen)\n",
        "# stopword khusus\n",
        "stopword_nat = {'timnas indonesia','main timnas','indonesia','main naturalisasi','naturalisasi','sepak bola',\n",
        "                'main','main turun','timnas','sepak','bola','sih','yg','u','iya','orang','nya','turun','keturunan',\n",
        "                'lokal','negara','darah','bangga','liga','maju','warga','pakai','tim','belanda'}\n"
      ],
      "metadata": {
        "id": "lGaiPm6QT2im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi Pembersih stopword\n",
        "def clean_text(text, stopwords):\n",
        "    # Mengubah teks menjadi huruf kecil\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenisasi menggunakan TextBlob\n",
        "    tokens = TextBlob(text).words\n",
        "\n",
        "    # Menghapus stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "    # Menggabungkan kembali token menjadi teks\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Membuat kolom baru tanpa stopword khusus\n",
        "df2['stem_teksnew'] = df2['stem_teks'].apply(lambda x: clean_text(x, stopword_nat))"
      ],
      "metadata": {
        "id": "YfUCx1-4Vgmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk memanggil baris yang mengandung beberapa keyword\n",
        "def filter_teks_multi_kata(df2, kolom, kata_kunci):\n",
        "    # Membuat filter untuk semua kata kunci\n",
        "    mask = df2[kolom].str.contains(kata_kunci[0], case=False, na=False)\n",
        "    for kata in kata_kunci[1:]:\n",
        "        mask &= df2[kolom].str.contains(kata, case=False, na=False)\n",
        "    return df2[mask]"
      ],
      "metadata": {
        "id": "EuZrXiP2XeJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WC sentimen positif"
      ],
      "metadata": {
        "id": "aJWSXcGWYrkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfpos = df2[df2['sentiment']==3]"
      ],
      "metadata": {
        "id": "42m0MTFLYw-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan kolom teks jika ada\n",
        "text = \" \".join(dfpos['stem_teksnew'])\n",
        "\n",
        "# Generate dan tampilkan word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_jWwSd7NYw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "Beberapa kata yang paling digunakan adalah ‘bela’, ‘asing’, ‘bagus’, salah’, ‘beda’, dan sebagainya. Netizen berpendapat bahwa naturalisasi sah secara aturan dan pemain asing keturunan Indonesia berhak membela timnas. Hal ini juga dianggap baik dalam upaya meningkatkan kualitas timnas."
      ],
      "metadata": {
        "id": "fXAkLiLgVxXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat isi tweet berdasarkan beberapa kata kunci yang besar\n",
        "dfposwhat = filter_teks_multi_kata(dfpos, 'stem_teksnew', ['bela','asing'])\n",
        "listpos = list(dfposwhat['full_text'])\n",
        "listurlpos = list(dfposwhat['tweet_url'])\n",
        "for i in range(len(listpos)):\n",
        "  print(f\"{i+1}. Link [{listurlpos[i]}] : {listpos[i]}\")"
      ],
      "metadata": {
        "id": "8vAQ2C0oYw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat isi tweet  berdasarkan beberapa kata kunci yang besar\n",
        "dfposwhat2 = filter_teks_multi_kata(dfpos, 'stem_teksnew', ['salah','dukung'])\n",
        "listpos2 = list(dfposwhat2['full_text'])\n",
        "listurlpos2 = list(dfposwhat2['tweet_url'])\n",
        "for i in range(len(listpos2)):\n",
        "  print(f\"{i+1}. Link [{listurlpos2[i]}] : {listpos2[i]}\")"
      ],
      "metadata": {
        "id": "eASX7lF_Yw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WC Sentimen Negatif"
      ],
      "metadata": {
        "id": "No1PUWn_Wslp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfneg = df2[df2['sentiment']==1]"
      ],
      "metadata": {
        "id": "ThHEw7qCXWe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan kolom teks jika ada\n",
        "text = \" \".join(dfneg['stem_teksnew'])\n",
        "\n",
        "# Generate dan tampilkan word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eoDOetJqWTG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "Beberapa kata yang paling digunakan adalah ‘asli’, ‘bina’, ‘banyak’, kritik’, ‘bodoh’, dan sebagainya. Netizen merasa tidak puas terhadap pengembangan pemain lokal (asli Indonesia) dan cemas terhadap identitas timnas yang bergantung pada pemain asing. Netizen juga menyerukan pembinaan yang lebih baik untuk penguatan identitas nasional timnas Indonesia."
      ],
      "metadata": {
        "id": "hxx8syUZWQJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat isi tweet berdasarkan beberapa kata kunci yang besar\n",
        "dfnegwhat = filter_teks_multi_kata(dfneg, 'stem_teksnew', ['bina','banyak'])\n",
        "listneg = list(dfnegwhat['full_text'])\n",
        "listurlneg = list(dfnegwhat['tweet_url'])\n",
        "for i in range(len(listneg)):\n",
        "  print(f\"{i+1}. Link [{listurlneg[i]}] : {listneg[i]}\")"
      ],
      "metadata": {
        "id": "awj3IqMqW0pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat isi tweet berdasarkan beberapa kata kunci yang besar\n",
        "dfnegwhat2 = filter_teks_multi_kata(dfneg, 'stem_teksnew', ['asli','isi'])\n",
        "listneg2 = list(dfnegwhat2['full_text'])\n",
        "listurlneg2 = list(dfnegwhat2['tweet_url'])\n",
        "for i in range(len(listneg2)):\n",
        "  print(f\"{i+1}. Link [{listurlneg2[i]}] : {listneg2[i]}\")"
      ],
      "metadata": {
        "id": "hukeVlBjX0-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat isi tweet berdasarkan beberapa kata kunci yang besar\n",
        "dfnegwhat3 = filter_teks_multi_kata(dfneg, 'stem_teksnew', ['rocky','dapat'])\n",
        "listneg3 = list(dfnegwhat3['full_text'])\n",
        "listurlneg3 = list(dfnegwhat3['tweet_url'])\n",
        "for i in range(len(listneg3)):\n",
        "  print(f\"{i+1}. Link [{listurlneg3[i]}] : {listneg3[i]}\")"
      ],
      "metadata": {
        "id": "av_hYRKGjeof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tren Sentimen (September)\n",
        "\n"
      ],
      "metadata": {
        "id": "LzZrd5NXMGHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan event_date\n",
        "event_date1 = pd.to_datetime('2024-09-05') # Tanggal Laga Pertama Indonesia di Kualifikasi Piala Dunia Putaran ketiga (Indonesia vs Arab)\n",
        "event_date = pd.to_datetime('2024-09-10') # Tanggal Laga Kedua Indonesia di Kualifikasi Piala Dunia Putaran ketiga (Indonesia vs Australia)\n",
        "\n",
        "# Fungsi untuk menentukan waktu (before, during, after)\n",
        "def classify_time(date, event_date,event_date1):\n",
        "    if date < event_date1: # Sebelum laga pertama\n",
        "        return 'before'\n",
        "    elif date.date() > event_date.date(): # Setelah laga kedua\n",
        "        return 'after'\n",
        "    else: # dalam interval antara laga pertama dan kedua\n",
        "        return 'during'\n",
        "\n",
        "# Terapkan fungsi pada kolom 'Tanggal' dan buat kolom baru 'Waktu'\n",
        "df2['Waktu'] = df2['created_at'].apply(lambda x: classify_time(x, event_date, event_date1))"
      ],
      "metadata": {
        "id": "8PsJG1HgMRsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah pengguna unik di setiap kombinasi Waktu dan Sentimen\n",
        "pivot_data = df2.groupby(['Waktu', 'sentiment_label'])['username'].nunique().unstack(fill_value=0)\n",
        "desired_order = ['before', 'during', 'after']\n",
        "pivot_data = pivot_data.reindex(desired_order)\n",
        "\n",
        "# Membuat line plot dengan jumlah pengguna unik\n",
        "pivot_data.plot(kind='line', marker='o', figsize=(8, 6))\n",
        "\n",
        "# Menambahkan judul dan label\n",
        "plt.title('Perbandingan Sentimen Sebelum, Selama, dan Setelah (Jumlah Pengguna Unik)')\n",
        "plt.xlabel('Waktu')\n",
        "plt.ylabel('Jumlah Pengguna Unik')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Sentimen', loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cNavHI31MLpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretasi:\n",
        "\n",
        "- “Sebelum” (before): Jumlah pengguna dengan sentimen negatif, netral, dan positif relatif rendah. Ini menunjukkan bahwa sebelum pertandingan dimulai, tingkat aktivitas dan opini publik masih belum tinggi.\n",
        "\n",
        "- “Selama” (during): Sentimen positif meningkat signifikan, diikuti oleh kenaikan sentimen netral dan sedikit peningkatan sentimen negatif. Peningkatan ini mencerminkan respons positif publik terhadap performa timnas selama pertandingan Indonesia vs Arab dan Indonesia vs Australia.\n",
        "\n",
        "- “Setelah” (after): Peningkatan terbesar terjadi pada setiap sentimen. Hal ini menunjukkan bahwa kedua pertandingan telah menghasilkan peningkatan aktivitas dan diskusi yang signifikan di platform Twitter (X) ini, menunjukkan bahwa dampak signifikan dari kedua pertandingan."
      ],
      "metadata": {
        "id": "WGQEKLqUWlMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kesimpulan"
      ],
      "metadata": {
        "id": "aUBVty8rWp48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Isu utama yang dibahas mencakup kegagalan pembinaan pemain lokal, ketergantungan pada pemain asing, dan dampaknya terhadap identitas tim nasional. Di sisi lain, dukungan terhadap kebijakan ini berfokus pada peningkatan kualitas permainan dan hak pemain keturunan untuk membela Timnas Indonesia.\n",
        "- Sentimen publik didominasi pandangan positif (34,3%), menyoroti manfaat naturalisasi untuk meningkatkan performa tim. Sebagian memiliki sentimen negatif (14,4%), dengan kritik utama pada lemahnya pembinaan pemain lokal. Sentimen netral juga cukup besar, mencerminkan opini berimbang.\n",
        "- Pola percakapan menunjukkan lonjakan diskusi signifikan pada 12-13 September, dipicu statement yang dikeluarkan oleh peter gontha menunjukkan ketidaksetujuan terhadap kebijakan PSSI terkait naturalisasi Timnas dan antusiasme publik dalam kualifikasi Piala Dunia.\n",
        "- Respons netizen menunjukkan pandangan yang beragam. Pendukung berpendapat bahwa kebijakan ini sah dan memperkuat Timnas, sementara kritik terutama datang dari kekhawatiran ketergantungan pada pemain asing dan seruan untuk memperbaiki pembinaan pemain lokal."
      ],
      "metadata": {
        "id": "o5sNaV4AW3UR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saran"
      ],
      "metadata": {
        "id": "cINAhSuJWzty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pembinaan Pemain Lokal : PSSI perlu meningkatkan program pembinaan usia muda dengan menyediakan fasilitas, pelatihan, dan kompetisi yang berkualitas. Pemerintah dapat mendukung melalui pendanaan dan kebijakan untuk memastikan pemain lokal memiliki peluang yang sama untuk berkembang seperti pemain asing.\n",
        "- Keseimbangan Kebijakan Naturaliasi : Kebijakan naturalisasi sebaiknya tidak menjadi solusi utama, melainkan pelengkap strategi jangka panjang. PSSI harus tetap memprioritaskan pembinaan pemain lokal sebagai tulang punggung tim nasional sambil menggunakan pemain naturalisasi untuk memperkuat posisi tertentu yang masih kekurangan talenta.\n",
        "- Peningkatan Transparansi Kebijakan Naturalisasi: Transparansi dalam proses naturalisasi, termasuk alasan memilih pemain tertentu dan dampaknya terhadap tim, akan membantu meredam kritik publik. PSSI dapat mengkomunikasikan tujuan strategis serta kebijakan ini kepada masyarakat dengan lebih jelas. Dan membuat masyarakat lebih terdukasi tentang alasan naturalisasi dan dampaknya dapat memperkuat dukungan terhadap kebijakan ini.\n",
        "- Pemanfaatan Pemain Naturalisasi sebagai Mentor Pemain Muda : Pelatih dapat mengoptimalkan kehadiran pemain naturalisasi dengan mendorong mereka menjadi mentor bagi pemain muda lokal, sehingga pengalaman dan keterampilan mereka dapat ditransfer kepada generasi berikutnya.\n",
        "- Kolaborasi Pemerintah dan PSSI : Pemerintah dapat mendukung pembinaan pemain lokal dengan meningkatkan anggaran pengembangan olahraga, memperbaiki infrastruktur sepak bola, dan memastikan pemerataan akses pelatihan di seluruh daerah.\n",
        "\n"
      ],
      "metadata": {
        "id": "ti8R82aQbZnK"
      }
    }
  ]
}